# -*- coding: utf-8 -*-
"""getting-started-ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19eg-qXLO6ywbDl66FJ9AQl46Wjvs1qja

# Getting started with PPO and ProcGen

Here's a bit of code that should help you get started on your projects.

Hyperparameters. 
These values should be a good starting point. You can modify them later once you have a working implementation.
"""
# Hyperparameters
total_steps = 1e3
num_envs = 32
num_levels = 10
num_steps = 256
num_epochs = 1
batch_size = 512
eps = .2
grad_eps = .5
clip_value = .1
value_coef = .5
entropy_coef = .01


"""
Network definitions. 
We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below),
while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures,
so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM).
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import make_env, Storage, orthogonal_init
from labml_nn.rl.ppo import ClippedPPOLoss, ClippedValueFunctionLoss

# from labml_helpers.module import Module
#from labml_nn.rl.ppo.gae import GAE

# if torch.cuda.is_available():
#     device = torch.device("cuda:0")
# else:
#     device = torch.device("cpu")

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class Encoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),
        Flatten(),
        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()
    )
    self.apply(orthogonal_init)

  def forward(self, x):
    return self.layers(x)

class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)
    
    return action.cpu(), log_prob.cpu(), value.cpu()

  def forward(self, x):
    x = self.encoder(x)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value

# class ClippedPPOLoss(Module):
#   def __init__(self):
#     super().__init__()
#   def forward(self, log_pi: torch.Tensor, sampled_log_pi: torch.Tensor,advantage: torch.Tensor, clip: float) -> torch.Tensor:
#     ratio = torch.exp(log_pi - sampled_log_pi)
#     clipped_ratio = ratio.clamp(min=1.0 - clip,max=1.0 + clip)
#     policy_reward = torch.min(ratio * advantage,clipped_ratio * advantage)
#     self.clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()
#     return -policy_reward.mean()

# class ClippedValueFunctionLoss(Module):
#   def forward(self, value: torch.Tensor, sampled_value: torch.Tensor, sampled_return: torch.Tensor, clip: float):
#     clipped_value = sampled_value + (value - sampled_value).clamp(min=-clip, max=clip)
#     vf_loss = torch.max((value - sampled_return) ** 2, (clipped_value - sampled_return) ** 2)
#     return 0.5 * vf_loss.mean()


# Define environment
# check the utils.py file for info on arguments
env = make_env(n_envs=num_envs,env_name='coinrun',num_levels=num_levels)
print('Observation space:', env.observation_space)
print('Action space:', env.action_space.n)

# Define network
# Define network
encoder = Encoder(in_channels=3, feature_dim=4096)
policy = Policy(encoder=encoder, feature_dim=4096, num_actions=env.action_space.n)
policy.cuda()

# Define optimizer
# these are reasonable values but probably not optimal
optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)

# Define temporary storage
# we use this to collect transitions during each iteration
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs
)

clipped_PPO_loss = ClippedPPOLoss()
clipped_value_loss = ClippedValueFunctionLoss()

# Run training
obs = env.reset()
step = 0
while step < total_steps:

  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps):
    # Use policy
    action, log_prob, value = policy.act(obs)
    
    # Take step in environment
    next_obs, reward, done, info = env.step(action)

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value)
    
    # Update current observation
    obs = next_obs

  # Add the last observation to collected data
  _, _, value = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  # Optimize policy
  policy.train()
  for epoch in range(num_epochs):

    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator:
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch

      # Get current policy outputs
      new_dist, new_value = policy(b_obs)
      new_log_prob = new_dist.log_prob(b_action)

      # Clipped policy objective
      pi_loss = clipped_PPO_loss(log_pi=new_log_prob, sampled_log_pi=b_log_prob, advantage=b_advantage, clip=clip_value)
      
      # Clipped value function objective
      value_loss = clipped_value_loss(value=new_value, sampled_value=b_value, sampled_return=b_returns, clip=clip_value)

      # Entropy loss
      entropy_loss = new_dist.entropy()
      entropy_loss = entropy_loss.mean()

      # Backpropagate losses
      loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss) 
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)

      # Update policy
      optimizer.step()
      optimizer.zero_grad()

  # Update stats
  step += num_envs * num_steps
  print(f'Step: {step}\tMean reward: {storage.get_reward()}')

print('Completed training!')
torch.save(policy.state_dict(), 'checkpoint.pt')

"""Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."""

env = make_env(n_envs=num_envs,env_name='coinrun',num_levels=num_levels)
encoder = Encoder(in_channels=3, feature_dim=4096)
policy = Policy(encoder=encoder, feature_dim=4096, num_actions=env.action_space.n)
policy.cuda()
policy.load_state_dict(torch.load('checkpoint.pt'))
import imageio

# Make evaluation environment
eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, env_name='coinrun')
obs = eval_env.reset()

frames = []
total_reward = []

# Evaluate policy
policy.eval()
for _ in range(512):

  # Use policy
  action, log_prob, value = policy.act(obs)

  # Take step in environment
  obs, reward, done, info = eval_env.step(action)
  total_reward.append(torch.Tensor(reward))

  # Render environment and store
  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
  frames.append(frame)

# Calculate average return
total_reward = torch.stack(total_reward).sum(0).mean(0)
print('Average return:', total_reward)

# Save frames as video
frames = torch.stack(frames)
imageio.mimsave('vid.mp4', frames, fps=25)